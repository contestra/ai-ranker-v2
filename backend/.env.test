# Test Suite Environment Configuration
# Apply ChatGPT's recommended settings to stabilize tests

# =======================
# OpenAI Rate Limiting
# =======================
# Reduce token budget to avoid 429 errors
OPENAI_MAX_OUTPUT_TOKENS_CAP=2000
OPENAI_DEFAULT_MAX_OUTPUT_TOKENS=1400

# Lower TPM limit below actual 30k for headroom
# This will be read by get_settings() and used by the rate limiter
# You may need to update your settings to read this from env
OPENAI_TPM_LIMIT=24000

# Force sequential execution for tests
OPENAI_MAX_CONCURRENCY=1

# =======================
# Timeout Configuration
# =======================
# Standard timeouts for ungrounded/grounded requests
LLM_TIMEOUT_UN=90
LLM_TIMEOUT_GR=240

# =======================
# Proxy Configuration
# =======================
# All proxy functionality has been removed
DISABLE_PROXIES=true

# =======================
# Test Execution Strategy
# =======================
# Recommended test order to minimize rate limit issues:
# 1. Run ungrounded tests first (lower token usage)
# 2. Run grounded tests second (higher token usage)
#
# All vantage_policy values with PROXY are automatically 
# normalized to ALS_ONLY (proxies disabled)

# =======================
# Additional Safeguards
# =======================
# If a single call uses >20k tokens, add extra sleep before next test
# Monitor actual token usage vs estimates to tune adaptive multiplier

