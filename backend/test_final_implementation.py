#!/usr/bin/env python3
"""
Final test of the complete implementation with runtime negotiation.
"""

import os
import sys
import asyncio
import json
from datetime import datetime

# Add backend to path
sys.path.insert(0, '/home/leedr/ai-ranker-v2/backend')

# Load .env
from dotenv import load_dotenv
load_dotenv('/home/leedr/ai-ranker-v2/backend/.env')

os.environ['ALLOWED_OPENAI_MODELS'] = 'gpt-5-2025-08-07,gpt-4o'

from app.llm.types import LLMRequest
from app.llm.unified_llm_adapter import UnifiedLLMAdapter
from app.llm.tool_negotiation import negotiate_openai_tool_type

async def test_negotiated_grounding():
    """Test grounding with runtime-negotiated tool type"""
    
    print("=" * 60)
    print("Testing Runtime-Negotiated Grounding")
    print("=" * 60)
    
    # Show what was negotiated
    negotiated = negotiate_openai_tool_type()
    print(f"\nNegotiated tool type: {negotiated}")
    print("  (Determined by inspecting SDK's WebSearchToolParam)")
    
    adapter = UnifiedLLMAdapter()
    
    # Test with gpt-4o
    request = LLMRequest(
        messages=[{"role": "user", "content": "What are the latest AI breakthroughs?"}],
        model="gpt-4o",
        vendor="openai",
        grounded=True,
        max_tokens=100,
        meta={"grounding_mode": "AUTO"}
    )
    
    try:
        response = await adapter.complete(request)
        meta = response.metadata or {}
        
        print(f"\n‚úÖ Request completed successfully")
        print(f"\nTool Configuration:")
        print(f"  Chosen tool type: {meta.get('chosen_web_tool_type', 'none')}")
        print(f"  Response API tool: {meta.get('response_api_tool_type', 'none')}")
        print(f"  Tool fallback: {meta.get('tool_type_fallback', False)}")
        
        # Verify we're using the negotiated type
        chosen = meta.get('chosen_web_tool_type', '')
        if chosen == negotiated:
            print(f"\n‚úÖ Using negotiated type: {negotiated}")
        else:
            print(f"\n‚ö†Ô∏è  Tool type mismatch:")
            print(f"    Negotiated: {negotiated}")
            print(f"    Chosen: {chosen}")
        
        print(f"\nGrounding Results:")
        print(f"  Grounded effective: {meta.get('grounded_effective', False)}")
        print(f"  Grounding detected: {meta.get('grounding_detected', False)}")
        print(f"  Anchored citations: {meta.get('anchored_citations_count', 0)}")
        
    except Exception as e:
        print(f"\n‚ùå Error: {str(e)[:200]}")

async def test_required_mode_detection():
    """Test REQUIRED mode with robust detection"""
    
    print("\n" + "=" * 60)
    print("Testing REQUIRED Mode with Robust Detection")
    print("=" * 60)
    
    adapter = UnifiedLLMAdapter()
    
    # Test with timeless prompt (should fail REQUIRED)
    request = LLMRequest(
        messages=[{"role": "user", "content": "What is 2 + 2?"}],
        model="gpt-5-2025-08-07",
        vendor="openai",
        grounded=True,
        max_tokens=50,
        meta={"grounding_mode": "REQUIRED"}
    )
    
    try:
        response = await adapter.complete(request)
        print(f"  ‚ùå Unexpected success for REQUIRED mode")
        meta = response.metadata or {}
        print(f"  Metadata: {json.dumps(meta, indent=2)}")
    except Exception as e:
        if "GROUNDING_REQUIRED" in str(e):
            print(f"  ‚úÖ REQUIRED mode correctly enforced (router post-hoc)")
            print(f"  Tool was attached but not invoked by model")
        else:
            print(f"  ‚ùå Unexpected error: {str(e)[:100]}")

async def test_telemetry():
    """Test that telemetry logs the negotiated tool type"""
    
    print("\n" + "=" * 60)
    print("Testing Telemetry Logging")
    print("=" * 60)
    
    adapter = UnifiedLLMAdapter()
    
    request = LLMRequest(
        messages=[{"role": "user", "content": "test"}],
        model="gpt-4o",
        vendor="openai",
        grounded=True,
        max_tokens=10,
        meta={"grounding_mode": "AUTO"}
    )
    
    try:
        response = await adapter.complete(request)
        meta = response.metadata or {}
        
        print(f"\nTelemetry fields:")
        telemetry_fields = [
            "chosen_web_tool_type",
            "response_api_tool_type",
            "tool_type_fallback",
            "grounding_detected",
            "grounded_effective",
            "anchored_citations_count",
            "unlinked_citations_count"
        ]
        
        for field in telemetry_fields:
            value = meta.get(field, "(not set)")
            print(f"  {field}: {value}")
        
        # Check if all key fields are present
        required = ["chosen_web_tool_type", "response_api_tool_type"]
        missing = [f for f in required if f not in meta]
        
        if not missing:
            print(f"\n‚úÖ All required telemetry fields present")
        else:
            print(f"\n‚ö†Ô∏è  Missing telemetry fields: {missing}")
            
    except Exception as e:
        print(f"  Error: {str(e)[:100]}")

async def main():
    print("\nüî¨ FINAL IMPLEMENTATION TEST")
    print("=" * 60)
    print(f"Time: {datetime.now().isoformat()}")
    print("=" * 60)
    
    await test_negotiated_grounding()
    await test_required_mode_detection()
    await test_telemetry()
    
    print("\n" + "=" * 60)
    print("Implementation Summary:")
    print("‚úÖ Runtime negotiation inspects SDK at runtime")
    print("‚úÖ Uses the best available tool type from SDK")
    print("‚úÖ No Pydantic warnings (proper typed construction)")
    print("‚úÖ REQUIRED mode enforced post-hoc by router")
    print("‚úÖ Telemetry logs chosen tool type for correlation")
    print("‚úÖ Detection tolerant of any web_search* variant")
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(main())