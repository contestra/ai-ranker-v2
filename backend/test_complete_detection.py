#!/usr/bin/env python3
"""
Comprehensive test of all detection improvements.
"""

import os
import sys
import asyncio
import json
from datetime import datetime

# Add backend to path
sys.path.insert(0, '/home/leedr/ai-ranker-v2/backend')

# Load .env
from dotenv import load_dotenv
load_dotenv('/home/leedr/ai-ranker-v2/backend/.env')

os.environ['ALLOWED_OPENAI_MODELS'] = 'gpt-5-2025-08-07,gpt-4o'

from app.llm.types import LLMRequest
from app.llm.unified_llm_adapter import UnifiedLLMAdapter
from app.llm.tool_detection import (
    detect_openai_websearch_usage,
    detect_vertex_grounding_usage,
    attest_two_step_vertex,
    normalize_tool_detection
)

async def test_openai_grounding():
    """Test OpenAI grounding with new detection"""
    print("=" * 60)
    print("Testing OpenAI with Robust Detection")
    print("=" * 60)
    
    adapter = UnifiedLLMAdapter()
    
    # Test with gpt-4o (sometimes works)
    request = LLMRequest(
        messages=[{"role": "user", "content": "What are the latest AI developments today?"}],
        model="gpt-4o",
        vendor="openai",
        grounded=True,
        max_tokens=100,
        meta={"grounding_mode": "AUTO"}
    )
    
    try:
        response = await adapter.complete(request)
        meta = response.metadata or {}
        
        print(f"\nOpenAI Response Metadata:")
        print(f"  Model: {response.model}")
        print(f"  Grounded effective: {meta.get('grounded_effective', False)}")
        print(f"  Tool type chosen: {meta.get('chosen_web_tool_type', 'none')}")
        print(f"  Tool type fallback: {meta.get('tool_type_fallback', False)}")
        print(f"  Tools requested: {meta.get('response_api_tool_type') is not None}")
        print(f"  Grounding detected: {meta.get('grounding_detected', False)}")
        print(f"  Anchored citations: {meta.get('anchored_citations_count', 0)}")
        
        # Use new detection on the response
        if hasattr(response, 'raw_response'):
            response_dict = response.raw_response
        else:
            response_dict = None
        
        if response_dict:
            tools_used, call_count, kinds = detect_openai_websearch_usage(response=response_dict)
            print(f"\n  New Detection Results:")
            print(f"    Tools used: {tools_used}")
            print(f"    Call count: {call_count}")
            print(f"    Tool kinds: {kinds}")
        
    except Exception as e:
        print(f"  Error: {str(e)[:200]}")

async def test_vertex_mock():
    """Test Vertex detection with mock response"""
    print("\n" + "=" * 60)
    print("Testing Vertex with Two-Step Detection")
    print("=" * 60)
    
    # Mock Vertex-style responses
    step1_response = {
        "candidates": [{
            "content": "Based on my search...",
            "grounding_metadata": {
                "grounding_chunks": [
                    {"web": {"uri": "https://ai-news.com/latest", "domain": "ai-news.com"}},
                    {"web": {"uri": "https://tech-blog.io/ai-2025", "domain": "tech-blog.io"}}
                ]
            }
        }]
    }
    
    step2_response = {
        "candidates": [{
            "content": '{"summary": "AI developments in 2025...", "key_points": ["point1", "point2"]}'
        }]
    }
    
    # Test detection
    tools_used, signal_count, signals, source_urls = detect_vertex_grounding_usage(response=step1_response)
    
    print(f"\nStep 1 Detection:")
    print(f"  Tools used: {tools_used}")
    print(f"  Signals: {signals}")
    print(f"  Source URLs: {source_urls}")
    
    # Test two-step attestation
    attestation = attest_two_step_vertex(
        step1_response=step1_response,
        step2_response=step2_response
    )
    
    print(f"\nTwo-Step Attestation:")
    print(f"  Contract OK: {attestation['contract_ok']}")
    print(f"  Step 1 tools: {attestation['step1_tools_used']}")
    print(f"  Step 1 sources: {attestation['step1_sources_count']}")
    print(f"  Step 2 tools: {attestation['step2_tools_used']}")
    
    # Test normalized detection
    result = normalize_tool_detection("vertex", response=step1_response)
    print(f"\nNormalized Detection:")
    print(f"  {json.dumps(result, indent=2)}")

async def test_required_mode():
    """Test REQUIRED mode enforcement with new detection"""
    print("\n" + "=" * 60)
    print("Testing REQUIRED Mode with Robust Detection")
    print("=" * 60)
    
    adapter = UnifiedLLMAdapter()
    
    # Test REQUIRED mode with timeless prompt (should fail)
    request = LLMRequest(
        messages=[{"role": "user", "content": "What is the capital of France?"}],
        model="gpt-5-2025-08-07",
        vendor="openai",
        grounded=True,
        max_tokens=50,
        meta={"grounding_mode": "REQUIRED"}
    )
    
    try:
        response = await adapter.complete(request)
        print(f"  Unexpected success for REQUIRED mode")
    except Exception as e:
        if "GROUNDING_REQUIRED" in str(e):
            print(f"  ‚úÖ REQUIRED mode correctly enforced")
            print(f"  Reason: {str(e).split(':', 1)[1][:100] if ':' in str(e) else str(e)[:100]}")
        else:
            print(f"  ‚ùå Unexpected error: {str(e)[:100]}")

async def main():
    print("\nüî¨ COMPREHENSIVE DETECTION TEST SUITE")
    print("=" * 60)
    print(f"Time: {datetime.now().isoformat()}")
    print("=" * 60)
    
    await test_openai_grounding()
    await test_vertex_mock()
    await test_required_mode()
    
    print("\n" + "=" * 60)
    print("‚úÖ All detection improvements tested successfully!")
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(main())